"""
Evaluation Task2: LLM Score
This script evaluates the comparative reasoning quality of legal judgment outputs generated by LLMs
using two different prompting strategies: Standard (STD) and Chain-of-Thought (COT).

It loads predicted outputs and gold-standard references, constructs evaluation prompts for an LLM-based
judge to score reasoning completeness and correctness. Scores are parsed and stored for each case,
and average scores across all cases are computed and reported.

Output scores are saved to a JSON file for later analysis.
"""
from openai import OpenAI
from tqdm import tqdm
import json
from pathlib import Path
import re
import os
import argparse


client = OpenAI(
    base_url="https://ark.cn-beijing.volces.com/api/v3",
    api_key="Your Deepseek API Key",
)

def evaluate(gold_file, pred_file):
    skipped = []
    results = []

    gold_files_all = [f for f in os.listdir(gold_file) if f.endswith('.json')]
    pred_files_all = [f for f in os.listdir(pred_file) if f.endswith('.json')]

    pred_all_files = os.listdir(pred_file)
    txt_skipped_files = [f for f in pred_all_files if f.endswith('.txt')]
    for f in txt_skipped_files:
        skipped.append({"gold": None, "pred": f, "reason": "txt file skipped"})

    gold_dict = {int(re.search(r'\d+', f).group()): f for f in gold_files_all}
    pred_dict = {int(re.search(r'\d+', f).group()): f for f in pred_files_all}

    common_keys = sorted(set(gold_dict.keys()) & set(pred_dict.keys()))
    gold_files = [gold_dict[k] for k in common_keys]
    pred_files = [pred_dict[k] for k in common_keys]

    for gf, pf in tqdm(list(zip(gold_files, pred_files)), total=len(gold_files), desc="评估"):
        with open(os.path.join(gold_file, gf), 'r', encoding='utf-8') as fg, \
             open(os.path.join(pred_file, pf), 'r', encoding='utf-8') as fp:
            try:
                gd = json.load(fg)
                pd = json.load(fp)
            except Exception as e:
                skipped.append({"gold": gf, "pred": pf, "reason": str(e)})
                print(f"Error loading {gf} or {pf}: {e}")
                continue

            reference_answer = (gd.get("案件分析", "") + "\n" + gd.get("最终判决", "")).strip()
            predicted_answers = [pd.get("std", ""), pd.get("cot", "")]

            prompt = f"""
你是一位法律专家评审员，正在对同一个AI模型在不同推理方式下生成的法律分析结果进行比较评估。模型应基于以下法律推理链条进行完整分析：

内幕信息形成 → 信息知悉 → 交易行为 → 违法所得 → 法律适用与判决类型 → 处罚结果

请根据以下标准，分别对STD与COT模型的推理质量进行评分：

- A（优秀）：推理完整覆盖上述六个链条节点，逻辑清晰严密，法律适用正确，结论合理；
- B（中等）：推理链基本合理，但部分环节有缺失或模糊，影响法律判断的严谨性；
- C（较差）：推理缺失严重，逻辑混乱或法律适用错误，无法形成有效裁决依据。

请严格按照如下格式返回两项得分（A、B 或 C）：
STD模型得分: <评分>
COT模型得分: <评分>

【STD模型输出】
{predicted_answers[0]}

【COT模型输出】
{predicted_answers[1]}

【参考答案】
{reference_answer}
"""
            

            response = client.chat.completions.create(
                model="deepseek-v3-250324",
                messages=[{"role": "user", "content": prompt}],
            )

            content = response.choices[0].message.content
            std_score_match = re.search(r"STD模型得分[:：]?\s*([ABC])", content)
            cot_score_match = re.search(r"COT模型得分[:：]?\s*([ABC])", content)
            std_score = std_score_match.group(1) if std_score_match else "N/A"
            cot_score = cot_score_match.group(1) if cot_score_match else "N/A"

            file_scores = {"filename": pf, "std_score": std_score, "cot_score": cot_score}
            results.append(file_scores)
            existing_results.append(file_scores)
            with open(json_path, "w", encoding="utf-8") as fw:
                json.dump(existing_results, fw, ensure_ascii=False, indent=2)

def compute_rscore(std_scores, cot_scores):
    score_map = {"A": 1, "B": 0.5, "C": 0}
    std_numeric = [score_map.get(s, 0) for s in std_scores]
    cot_numeric = [score_map.get(s, 0) for s in cot_scores]

    def safe_mean(lst):
        return sum(lst) / len(lst) if lst else 0

    mean_std = safe_mean(std_numeric)
    mean_cot = safe_mean(cot_numeric)

    diff = mean_cot - mean_std
    return mean_std, mean_cot, diff

def main(score_file):
    with open(score_file, "r", encoding="utf-8") as f:
        data = json.load(f)
    std_scores = [item["std_score"] for item in data if item["std_score"] in ["A","B","C"]]
    cot_scores = [item["cot_score"] for item in data if item["cot_score"] in ["A","B","C"]]

    mean_std, mean_cot, diff = compute_rscore(std_scores, cot_scores)
    print(f"Average Std input score: {mean_std:.2f}")
    print(f"Average CoT input score: {mean_cot:.2f}")
    print(f"Difference (CoT - Std): {diff:.2f}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Evaluate LLMScore Task2")
    parser.add_argument("--gold_dir", type=str, default="../data/processed", help="Directory containing gold standard JSON files")
    parser.add_argument("--pred_dir", type=str, default="../output/task2", help="Directory containing prediction JSON files")
    parser.add_argument("--eval_scores_path", type=str, default="evaluation_scores.json", help="Path to evaluation scores JSON file")
    parser.add_argument("--summary_only", action="store_true", help="Only summarize scores without running evaluation")
    args = parser.parse_args()

    json_path = args.eval_scores_path
    if not os.path.exists(json_path):
        with open(json_path, "w", encoding="utf-8") as fw:
            json.dump([], fw)
    try:
        with open(json_path, "r", encoding="utf-8") as fr:
            existing_results = json.load(fr)
    except json.JSONDecodeError:
        print("⚠️ Invalid JSON format detected in existing file. It will be reset to an empty list.")
        existing_results = []

    if not args.summary_only:
        evaluate(args.gold_dir, args.pred_dir)
    main(json_path)