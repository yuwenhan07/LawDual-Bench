# LawDual-Bench: A Dual-Task Benchmark and Chain-of-Thought Impact Study for Legal Reasoning

<div align="center">
  <img src="figs/lawdual bench.png" alt="LawDual Bench" width="60%">
</div>

随着大语言模型（LLMs）在法律应用中的快速发展，系统评估其在**法律文档处理**和**判决预测中的推理能力**变得尤为迫切。目前公开的法律测评基准缺少统一的评估架构，对这两个任务的支持并不好。为填补这一空白，我们提出了 **LawDual-Bench**，填补了中文法律自然语言处理领域中结构化推理评估的关键空白，并为法律垂类大模型系统的评估与优化提供了坚实基础。更多详情可查看我们的论文。


## 📄 介绍
LawDual-Bench 经精心设计，可以对大模型的**法律文档理解和案情分析推理能力**进行精确评估。我们设计了一套半自动化的数据集构建方案，通过**人工+LLM**的方式，构建了一个全面的内幕交易数据集，同时也可以很轻易地扩展数据集的数量与案情的种类。再次基础上，我们设计了**结构化信息抽取** 和 **案件事实分析与判决预测** 两个任务。为了更好地评估模型在这两个任务上的性能，我们设计了详细的不同评估方式，评估结果**精确全面**。同时，我们通过人工专家设计了一条**高质量的CoT思维链**，希望测试模型在得到推理链的情况下推理能力是否能有所提升，与人类司法过程是否有更强的一致性。

## 📖 数据来源与构建流程

### 数据来源
LawDual-Bench 的数据主要来自以下三个公开渠道，聚焦于2005年至2024年间的**内幕交易案例**，覆盖行政案件、刑事案件及不起诉案件等法律阶段：

- 中国证券监督管理委员会（CSRC）发布的行政处罚决定书；
- 中国裁判文书网公布的刑事判决书；
- 全国检察机关公开的不起诉决定书。

每份原始法律文书以 `.docx`、`.pdf` 或 `.html` 格式采集，并转换为结构化文本进行统一处理，同时记录关键信息如**文书编号、裁决日期与来源链接**。我们删去了一些没有给出具体案件描述与结果的公开文书，保留了有效的部分。

### 结构化数据设计
我们与法律专家合作，制定了统一的结构化字段体系，详见`data/schema.json`，涵盖内幕信息识别、当事人基本信息、交易行为、非法所得计算、适用法条及最终处罚结果等六大维度。所有字段严格对齐《中华人民共和国证券法》《中华人民共和国刑法》《民法典》等相关条款，确保数据的**法律一致性、语义可比性**与司法推理的结构化支持能力。

### 数据标注过程
#### 人工标注
在第一阶段，我们对超过 **1000份真实案例文书**进行了逐项人工标注，依据结构模板标注超过 **50,000 个字段**。所有标注由具备法律背景的专业人员完成，并设置了**交叉复核与抽样质检机制**，以确保一致性与法律解释的准确性。
#### LLM辅助抽取 + 人工核验
在初步建立高质量种子集后，我们扩展性地处理了 **400 余份新增案例文书（2020–2024年）**。在实验对比不同大模型抽取策略后，最终选用性能与性价比兼优的 DeepSeek-V3 作为自动抽取工具。为提升其在法律任务下的适应能力，我们对抽取 prompt 模板进行了专门优化。所有自动输出均由法律专家逐字段进行**人工复核**，确保结构完整与语义准确。

![data contruction](figs/data-contruction.png)

### 数据统计概览

| 指标 | 数值 |
|------|------|
| 时间跨度 | 2005–2024 |
| 案例总数 | 1389 |
| 每份文书平均字段数 | 43.03 |
| 总结构化字段条目 | 59,771 |
| 核心字段平均填写率 | 86.58% |

### 数据格式
所有的原始和结构化数据都储存在 `data` 文件夹下，处理后的数据都以json文件格式存储，具体见 `data/processed` ，可以通过 `json.load` 的方式加载使用。其中`input.json` 文件是汇总了每个json文件中作为输入的部分，方便使用者调用。

## 🧩 双任务定义

### Task 1：结构化信息抽取（Structured Information Extraction）

该任务旨在从法律案情段落中提取标准化的关键字段，模拟法律专业人士阅读文书后的要素归纳过程。字段涵盖内幕信息的认定、当事人信息、交易行为、违法所得、适用法律条款与处罚结果等，具体字段定义见 `data/extract_schema.json`。

- 输入：原始法律文书中的案情描述段（自然语言文本）
- 输出：结构化 JSON 格式，填充各类字段信息
- 评估指标：
  - Field Accuracy：字段值的严格匹配准确率
  - Semantic Accuracy：字段语义相似度判断的匹配率
  - Overall Accuracy：上述两项的加权综合评分
  - Field Completeness Rate (FCR)：输出结构的字段覆盖与格式完整性

📌 评估指标的详细计算方式参见论文中第 4.1 节


### Task 2：事实-裁判推理任务（Fact-to-Judgement Reasoning）

此任务关注模型能否基于案情描述生成逻辑严密、结构完整的法律分析过程与最终判决，评估其推理质量与法律要素的覆盖情况。

- 输入模式：
  - 标准输入（Std）：仅提供案件描述，让模型自主完成分析与判断
  - 链式输入（CoT）：提供案件描述 + 结构化推理提示，引导模型按“事实识别 → 法律适用 → 裁判结果”的顺序推理
- 输出形式：自然语言撰写的完整分析过程，涵盖核心事实、法律条款适用与裁判结论
- 评估指标：
  - LLM Score：由高性能大语言模型依据逻辑性、完整性和合法性对输出评分（等级制：A/B/C）
  - Relative Score：衡量模型输出与人工标注结构字段的一致性程度（字段级匹配）

📌 具体评估流程与评分标准详见论文第 4.2 节


#### CoT 推理模板构建
专业法律从业者为模型设计了一套高质量的 Chain-of-Thought（CoT）提示词模板，专门用于引导模型在内幕交易案件中进行类比于司法实践的逐步推理。
在设计过程中，充分参考了《中华人民共和国刑法》《中华人民共和国民法典》《证券法》等相关法律条文，确保推理链在法律适用与表述上的**严谨性与权威性**。同时，我们邀请具有多年审判实务经验的法律从业者参与模板构建，依据**真实裁判逻辑**抽象出司法机关在处理内幕交易案件时普遍遵循的推理路径。
```text
内幕信息形成 → 信息知悉 → 交易行为 → 非法所得 → 法律适用 → 处罚决定
```

## 📊 实验结果
在实验中，我们评估了三大类模型在两个任务上的性能表现.
此外，我们在 `example/` 目录中提供了一个示例下不同模型输出结果，直观展示不同模型在实际法律文本处理中的差异与能力边界。

### 实验模型
- **通用大模型（General-purpose LLMs）**：如 GPT-4、Qwen2.5、GLM4、DeepSeek-V3 等，具备全面的文本理解与生成能力;
- **法律领域模型（Legal-domain LLMs）**：如 CleverLaw、Lawyer-LLM 等，针对法律语料微调，具有更强的专业性;
- **推理增强模型（Reasoning-augmented LLMs）**：如 DeepSeek-R1、QwQ-32B 等，通过引入快慢思维机制，拥有更强的推理能力。

### 各类大模型在 Task 1（结构化信息抽取）中的表现对比
![alt text](figs/task1_plot.png)
| Category         | Model                     | Filed Acc | Semantic Acc | Overall Acc | FCR      |
|------------------|---------------------------|-----------|---------------|--------------|----------|
| **General LLMs** | Llama3-8B-Chinese         | 57.24%    | 58.06%        | 57.43%       | **76.37%** |
|                  | GLM4-9B                   | 68.49%    | 60.59%        | 67.01%       | 68.85%   |
|                  | Qwen2.5-7B                | 56.65%    | 62.22%        | 57.69%       | 58.12%   |
|                  | Qwen2.5-72B               | 56.68%    | 67.37%        | 58.69%       | 57.26%   |
|                  | Llama3.3-70B-Instruct     | 65.66%    | 60.64%        | 64.72%       | 57.19%   |
|                  | Deepseek-V3               | 81.64%    | **69.83%**    | **77.43%**   | 69.65%   |
|                  | GPT-4                     | **82.36%**| 69.62%        | 77.23%       | 73.28%   |
|                  | GPT-4o                    | 73.84%    | 62.92%        | 72.43%       | 71.33%   |
| **Legal LLMs**   | AIE-51-8-law              | 34.84%    | **74.97%**    | 44.28%       | 29.67%   |
|                  | Cleverlaw                 | **61.71%**| 62.11%        | **61.79%**   | **78.43%** |
|                  | Law-Justice               | 35.30%    | 59.41%        | 39.32%       | 3.87%    |
|                  | Lawyer-LLM                | 57.87%    | 55.70%        | 57.49%       | 70.13%   |
| **Reasoning LLMs**| Deepseek-distill-qwen-1.5b| 47.42%   | 57.48%        | 48.60%       | 32.10%   |
|                  | Deepseek-distill-qwen-7b  | 62.66%    | 57.57%        | 61.71%       | **75.67%** |
|                  | Qwq-32b                   | 66.43%    | 62.73%        | 65.74%       | 71.51%   |
|                  | Deepseek-distill-llama-70B| 63.50%    | 61.51%        | 63.17%       | 52.72%   |
|                  | Deepseek-R1               | 74.69%    | 56.83%        | 70.72%       | 70.24%   |
|                  | GPT-o1-mini               | **80.89%**| **64.82%**    | **76.45%**   | 64.99%   |
|                  | GPT-o3-mini               | 78.35%    | 61.59%        | 74.39%       | 66.19%   |

### 各模型在 Task 2（法律事实分析与判决预测）中标准输出与 CoT 输出对比
![alt text](figs/task2_plot_relscore.png)
![alt text](figs/task2_plot_llmscore.png)
| Category         | Model               | Std Rel. | Std LLM | A (Std) | B (Std) | C (Std) | CoT Rel. | CoT LLM | A (CoT) | B (CoT) | C (CoT) | ΔRel. | ΔLLM  |
|------------------|---------------------|----------|----------|---------|---------|---------|-----------|----------|---------|---------|---------|--------|--------|
| **General LLMs** | Llama3-8B-Chinese   | 46.76%   | 29.63%   | 58      | 707     | 624     | 52.93%    | 60.37%   | 554     | 569     | 266     | 6.17%  | 30.74% |
|                  | GLM4-9B             | 50.79%   | 52.38%   | 191     | 1073    | 125     | 57.00%    | 70.88%   | 763     | 443     | 183     | 6.21%  | 18.50% |
|                  | Qwen2.5-7B          | 56.78%   | 65.73%   | 522     | 782     | 85      | 60.70%    | 65.44%   | 593     | 632     | 164     | 3.92%  | -0.29% |
|                  | Qwen2.5-72B         | 61.25%   | 68.50%   | 628     | 647     | 114     | 62.09%    | 57.70%   | 479     | 645     | 265     | 0.84%  | -10.80% |
|                  | Llama3-70B-Instruct | 53.47%   | 58.06%   | 309     | 995     | 85      | 57.87%    | 79.73%   | 892     | 431     | 66      | 4.40%  | 21.67% |
|                  | Deepseek-V3         | **65.66%**| **79.99%**| 854   | 514     | 21      | 69.00%    | 68.11%   | 651     | 590     | 148     | 3.34%  | -11.88% |
|                  | GPT-4               | 63.99%   | 61.09%   | 390     | 917     | 82      | 66.46%    | 70.99%   | 764     | 444     | 181     | 2.47%  | 9.90%  |
|                  | GPT-4o              | 66.06%   | 68.47%   | 554     | 794     | 41      | **71.78%**| **88.98%**| 1097   | 278     | 14      | 5.72%  | 20.52% |
| **Legal LLMs**   | AIE-51-8-law        | 39.41%   | 14.90%   | 6       | 402     | 981     | 39.65%    | 21.74%   | 61      | 482     | 846     | 0.24%  | 6.84%  |
|                  | Cleverlaw           | 41.86%   | 40.21%   | 46      | 1025    | 318     | 43.48%    | 43.63%   | 371     | 470     | 548     | 1.62%  | 3.42%  |
|                  | Law-Justice         | 39.83%   | 32.11%   | 26      | 840     | 523     | 41.06%    | 42.69%   | 333     | 520     | 536     | 1.23%  | 10.58% |
|                  | Lawyer-LLM          | **45.86%**| **27.65%**| 29    | 710     | 650     | **52.56%**| **65.15%**| 712     | 386     | 291     | 6.70%  | 37.51% |
| **Reasoning LLMs**| Deepseek-qwen-1.5b | 43.38%   | 35.82%   | 52      | 891     | 446     | 44.24%    | 44.28%   | 364     | 520     | 523     | 0.86%  | 8.46%  |
|                  | Deepseek-qwen-7b    | 47.70%   | 58.14%   | 415     | 785     | 189     | 52.88%    | 40.86%   | 223     | 689     | 477     | 5.18%  | -17.28% |
|                  | Qwq-32b             | 65.74%   | **93.70%**| 1237  | 129     | 23      | 62.54%    | 59.90%   | 334     | 996     | 59      | -3.20% | -33.96% |
|                  | Deepseek-llama-70B  | 51.72%   | 73.54%   | 729     | 585     | 75      | 55.05%    | 53.31%   | 323     | 835     | 231     | 3.33%  | -20.23% |
|                  | Deepseek-R1         | 65.81%   | **94.82%**| 1251  | 132     | 6       | 67.61%    | 62.71%   | 388     | 966     | 35      | 1.80%  | -32.11% |
|                  | GPT-o1-mini         | **72.49%**| 69.69%   | 631     | 674     | 84      | 72.08%    | 55.51%   | 462     | 618     | 309     | -0.41% | -14.18% |
|                  | GPT-o3-mini         | 63.55%   | 82.87%   | 915     | 472     | 2       | 67.37%    | **84.52%**| 986    | 376     | 27      | 3.82%  | 1.66%  |



## 🔧 如何评估模型
本项目提供了一个高质量、结构化的中文法律判决基准，覆盖“结构化信息抽取”与“法律事实分析与判决预测”双任务，并测试了 Chain-of-Thought 提示对模型推理效果的影响。

### 环境准备
1. **Python版本：** 建议使用Python ≥ 3.10
2. **安装依赖**
```bash
pip install -r requirements.txt
```
3. **准备模型文件：**
    - 将待评估的大语言模型文件或配置于 `model/` 目录
    - 下载中文法律语义嵌入模型 ChatLaw-Text2Vec，并放置于 `embedding_model/` 路径下，用于语义相似度计算


## 执行任务流程
### 运行Task 1 结构化信息抽取
```bash
python script/task1_extract.py \
  --model_path /path/to/model \
  --data_path ./data/input_data.json \
  --output_dir ./output/task1
```

### 运行Task 2 法律事实分析与判决预测
```bash
python script/task2_predict.py \
  --model_path /path/to/model \
  --data_path ./data/input_data.json \
  --output_dir ./output/task2
```
> 在运行Task2的过程中会将Std输出和CoT输出一起保存到输出的json文件中，如果要修改CoT，请直接修改python文件中的prompt

### 模型评估脚本
#### 评估Task 1 结构化信息抽取
**1. Overall Score（字段准确率 + 语义相似度）** 
```bash
python script/evaluate_Overall_task1.py \
  --gold_file data/processed \
  --pred_file output/task1 \
  --embedding_model embedding_model/ChatLaw-Text2Vec \
  --semantic_threshold 0.6
```

**2. FRC Score（字段完整率）**
```bash
python script/evaluate_FRC_task1.py \
  --data_dir ./output/task1 \
  --gold_dir ./data/processed
```
#### 评估Task 2 法律事实分析与判决预测
**1. LLM Score（推理质量等级A/B/C，由模型审阅）**
```bash
python script/evaluate_LLMScore_task2.py \
  --gold_dir data/processed \
  --pred_dir output/task2 \
  --eval_scores_path result/llm_score_eval.json
```

**2.Relative Score（推理输出与结构字段的一致性）**
```bash
python script/evaluate_RelScore_task2.py \
  --gold_dir data/processed \
  --pred_dir output/task2 \
  --embedding_model embedding_model/ChatLaw-Text2Vec \
  --threshold 0.6 \
  --output_path result/relscore_task2.json
```




## 📎 引用方式

如果您使用了 **LawDual-Bench** 数据集或代码，请引用我们的论文：

> *LawDual-Bench: A Dual-Task Benchmark and Chain-of-Thought Impact Study for Legal Outcomes*  
> （论文正在审稿中）

**注意**：该论文目前仍处于审稿阶段，尚未正式公开发表，发表后会在此更新。


## 🛡️ 声明

LawDual-Bench 所有法律数据均采自公开渠道，严格匿名化处理，仅用于研究用途，严禁用于真实法律判断。